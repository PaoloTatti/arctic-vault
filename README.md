# Artic Vault

The application was develop to participate in the Hackathon, Future of AI.

For more information, please reach out to:
* [Florian](florian.dahlmann@tui.co.uk)
* [Paolo](paolo.tatti@tui.be)
* [Paulo](paulo.fernandes@tui.com)

## Welcome to Arctic Vault!

Here you will find all the code and the Streamlit application to play around with Arctic Vault, your personal assistant able to help you generate efficient and optimized Data Vault 2.0.

The application is divided in three steps:
1. **Arctic Data Analyst**: This is the initial phase where the user uploads CSV files containing the necessary tables for their Data Vault. The Arctic Data Analyst helps the user evaluate the current data landscape. It checks for missing tables, identifies relationships, and ensures that all necessary components are present. This phase involves a detailed analysis to help the user understand any gaps or issues in their data structure, setting a solid foundation for the next steps.

2. **Arctic Data Architect**: Building on the insights from the Data Analyst, this agent engages in a deeper conversation with the user. It compiles and synthesizes information from the initial analysis to produce a comprehensive report. This report includes all the detailed specifications and schemas required to generate SQL queries for the Data Vault 2.0 tables. The Data Architect ensures that the data model adheres to best practices and optimizes for performance and scalability.

3. **Arctic Data Engineer**: In the final phase, the Data Engineer receives the detailed report generated by the Data Architect. This agent is responsible for translating the report into actionable SQL queries. For every Hub, Link, and Satellite table outlined in the report, it produces the necessary SQL code. These queries are crafted to be executed directly on Snowflake, creating the structured and optimized Data Vault 2.0.

